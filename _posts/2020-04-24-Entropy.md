---
layout: post
title: Entropy/Cross Entropy
date: 2020-04-24
author: Brad
tags: [Math, DNN Basic]
comments: true
toc: true
pinned: false
---



## 資訊量
啟源於訊息編碼，機器學習可用於描述機率分布或量化機率分布的相似性。比如不太可能發生的事發生了，要比非常可能發生的事，提供更多訊息。  
<!-- more -->

>定義一數，當某件事情機率越大，該數越小，表示為訊息量

$$
I(x)=-log(P(x)), x\in X\\
\text{以e為底稱nats}  \\
\text{以2為底稱bit/shannons}
$$

例子：小明平時不愛學習，考試經常不及格，而小王是個用功的好學生，經常滿分。  
我們可以假設事件A=小明考試及格，機率為$P(x_a)=0.1$，資訊量可寫為$I(x_a)=-log(0.1)=3.3219$  
事件B=小王考試及格，其機率$P(x_b)=0.9999$，資訊量為$I(x_b)=-log(0.9999)=0.0014$  
這告訴我們小明平常不及格突然及格了，一定發生了什麼事，資訊量大

## 熵
上述資量只能對單一因素或機率表示一個數值，所以
>定義熵用來表示整個系統相對於每個因素或機率的資訊量

$$
H(x)=\mathbb{E}[I(x)]=-\sum{p(x) log\left(p(x)\right)}
$$

## 相對熵、KL散度
`relative entropy, Kullback-Leibler divergence`  
表示P分布與Q分布的近似程度，P通常為樣本、Q為預測結果

$$
D_{KL}(P\Vert Q)=\mathbb{E}\left[log\frac{P(x)}{Q(x)}\right]=\sum_{x\in X}{P(x)log\frac{P(x)}{Q(x)}}
$$


## 交叉熵
`cross entropy`

$$
H(p,q)=-\sum{p(x)\log{(q(x))}}=-\sum{答案log(預測)}\\
H(p,q)=H(p)+D_{KL}(p\Vert q)
$$

它被當作損失函數的原因：
* 良好的誤差性質，預測與答案相同值為零，否則差越多，值越大
* 相較於MSE能避免學習速率下降問題(MSE兩端微分後值較小)

$$
MSE:= \frac{(y-\hat{y})^2}{2} \Rightarrow \frac{\partial (MSE)}{\partial w}=(\hat{y}-y)\sigma'(z)x\\
CE:= y\ln{\hat{y}}+(1-y)\ln{1-\hat{y}}\Rightarrow \frac{\partial (CE)}{\partial w}=(\sigma(z)-y)x
$$
